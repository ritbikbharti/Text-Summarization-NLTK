{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT SUMMARY USING NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _MINOR PROJECT BY RITBIK BHARTI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT IMPORTANT LIBRARIES\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "On 23 November 2008, the first public acknowledgement of an unmanned mission to Mars was announced by\n",
    "then-ISRO chairman G. Madhavan Nair.The MOM mission concept began with a feasibility study in 2010 by\n",
    "the Indian Institute of Space Science and Technology after the launch of lunar satellite Chandrayaan-1\n",
    "in 2008. Prime Minister Manmohan Singh approved the project on 3 August 2012, after the Indian Space\n",
    "Research Organisation completed ₹125 crore (US$18 million) of required studies for the orbiter.The\n",
    "total project cost may be up to ₹454 crore (US$66 million). The satellite costs ₹153 crore (US$22 million)\n",
    "and the rest of the budget has been attributed to ground stations and relay upgrades that will be used for\n",
    "other ISRO projects. The space agency had planned the launch on 28 October 2013 but was postponed to 5 November\n",
    "following the delay in ISRO's spacecraft tracking ships to take up pre-determined positions due to poor\n",
    "weather in the Pacific Ocean. Launch opportunities for a fuel-saving Hohmann transfer orbit occur every 26\n",
    "months, in this case the next two would be in 2016 and 2018. Assembly of the PSLV-XL launch vehicle, designated\n",
    "C25, started on 5 August 2013. The mounting of the five scientific instruments was completed at Indian Space\n",
    "Research Organisation Satellite Centre, Bengaluru, and the finished spacecraft was shipped to Sriharikota on\n",
    "2 October 2013 for integration to the PSLV-XL launch vehicle. The satellite's development was fast-tracked\n",
    "and completed in a record 15 months. Despite the US federal government shutdown, NASA reaffirmed on 5 October\n",
    "2013 it would provide communications and navigation support to the mission. During a meeting on 30 September\n",
    "2014, NASA and ISRO officials signed an agreement to establish a pathway for future joint missions to explore Mars.\n",
    "One of the working group's objectives will be to explore potential coordinated observations and science analysis\n",
    "between the MAVEN orbiter and MOM, as well as other current and future Mars missions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SENTENCES\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET STOPWORDS INCLUDING PUNCTUATIONS\n",
    "stopwords = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we’ll tokenise words. We also calculate word frequency. This measure will give us a metric to calculate if a word is important on the corpus. Here, we’ll use TF-IDF. TF-IDF is a metric that takes into account term frequency both in a single document and in the all corpus. So, a high TF-IDF occurs when a term has high frequency in a single document and low frequency on the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(WordNetLemmatizer().lemmatize(item))\n",
    "    return stems\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, \n",
    "                        stop_words=stopwords)\n",
    "tfs = tfidf.fit_transform([text])\n",
    "\n",
    "# FREQUENCIES\n",
    "freqs = {}\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in tfs.nonzero()[1]:\n",
    "    freqs[feature_names[col]] = tfs[0, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will calculate which are the most important sentences based on the presence of important words on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "important_sentences = defaultdict(int)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for token in word_tokenize(sentence.lower()):\n",
    "        if token in freqs:\n",
    "            important_sentences[i] += freqs[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now last step is to build the summary. An important thing to do here is to choose how many sentences will be present in our summary. I have decided to build a summary with 10% of the original number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sumary:\n",
      "\n",
      "\n",
      "On 23 November 2008, the first public acknowledgement of an unmanned mission to Mars was announced by\n",
      "then-ISRO chairman G. Madhavan Nair.The MOM mission concept began with a feasibility study in 2010 by\n",
      "the Indian Institute of Space Science and Technology after the launch of lunar satellite Chandrayaan-1\n",
      "in 2008.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "import operator\n",
    "\n",
    "# Choose 10% of the text to show\n",
    "number_sentences = int(len(sentences) * 0.10)\n",
    "\n",
    "# Create an index with the most important sentences\n",
    "index_important_sentences = nlargest(number_sentences, \n",
    "                                   important_sentences, \n",
    "                                   important_sentences.get)\n",
    "\n",
    "    \n",
    "# Create summary\n",
    "print('\\nSumary:\\n')\n",
    "for i in sorted(index_important_sentences):\n",
    "    print(sentences[i]+'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
